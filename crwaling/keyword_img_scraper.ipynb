{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import dload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "error",
     "timestamp": 1643171758355,
     "user": {
      "displayName": "Chung Ching Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13376823771419181326"
     },
     "user_tz": -480
    },
    "id": "FmlsKYXrJa4q",
    "outputId": "dd99b5ca-9ae6-4437-ec69-c82f98645f71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:43<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\jsyu\\anaconda3\\envs\\bworm\\lib\\site-packages\\numpy\\lib\\shape_base.py:652: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(v)\n"
     ]
    }
   ],
   "source": [
    "def scrap():\n",
    "    titles = []\n",
    "    authors = [] #store the song names\n",
    "    tags = []\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    for j in tqdm(range(1, 21)): #20page\n",
    "        \n",
    "        url = f\"https://search.kyobobook.co.kr/web/search?vPstrKeyWord=%EB%AF%BC%EC%9D%8C%EC%82%AC%20%EC%84%B8%EA%B3%84%EB%AC%B8%ED%95%99%EC%A0%84%EC%A7%91&searchPcondition=1&searchCategory=%EA%B5%AD%EB%82%B4%EB%8F%84%EC%84%9C@KORBOOK&collName=KORBOOK&from_CollName=%EA%B5%AD%EB%82%B4%EB%8F%84%EC%84%9C@KORBOOK&vPstrTab=PRODUCT&from_coll=KORBOOK&searchOrder=4&searchStatus=0&currentPage={j}&orderClick=LAH\"\n",
    "        r= scraper.get(url)\n",
    "        soup=BeautifulSoup(r.text,\"html.parser\")\n",
    "\n",
    "        #start extracting info\n",
    "        #all info are inside <tbody> </tbody>\n",
    "        #inside <tbody>, EACH song is encapsulated in a <tr></tr>\n",
    "        #inside <tr>,  the song name and singer is encapsulated inside <td class=\"chart-table-track\">: <strong> encapsulates the song name\n",
    "                                                           #   <span> encapsulates the singer\n",
    "        #inside <tr>,  <td class_=\"chart-table-streams\"> stores the streams \n",
    "        whole_table = soup.find(\"tbody\",id=\"search_list\").find_all(\"tr\")                                                \n",
    "\n",
    "        for k,i in enumerate(whole_table):\n",
    "            urllib.request.urlretrieve(i.find(\"div\",class_=\"cover\").find(\"img\")['src'],f'./imgs/{j}_{k}.jpg')\n",
    "            titles.append(i.find(\"div\", class_=\"title\").find(\"strong\").text)\n",
    "            authors.append(i.find(\"div\", class_=\"author\").find(\"a\").text)\n",
    "            if i.find(\"div\",class_=\"tag\") is None:\n",
    "                tags.append([''])\n",
    "            else:\n",
    "                tags.append([t.string for t in i.find(\"div\", class_=\"tag\").find_all(\"a\")])\n",
    "        \n",
    "                \n",
    "    return titles, authors, tags\n",
    "\n",
    "\n",
    "#########################################\n",
    "#put the result into a csv file\n",
    "#########################################\n",
    "data = scrap()\n",
    "df = pd.DataFrame(np.column_stack(data),columns=['title','author','tag'])\n",
    "df.to_csv('tags.csv',index=False)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spotify_chart_weekly_scraper.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
